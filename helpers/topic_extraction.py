import time
import json
import openai
from openai import OpenAI

client = OpenAI()

def call_api_with_retry(prompt: str, model: str, max_tokens: int, temperature: float, debug: bool = False) -> str:
    """
    Call the ChatCompletion API with retry logic for rate limiting.
    Returns the model's response content as a string.
    """
    retries = 0
    max_retries = 5
    while retries < max_retries:
        try:
            response = client.chat.completions.create(model=model,
            messages=[
                {"role": "system", "content": "You are an AI that extracts key topics from transcript texts."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            temperature=temperature)
            return response.choices[0].message.content.strip()
        except openai.RateLimitError as e:
            sleep_time = 2 ** retries
            if debug:
                print(f"Rate limit hit; retrying in {sleep_time} seconds...")
            time.sleep(sleep_time)
            retries += 1
    raise Exception("API call failed after retries.")

def get_topics_for_chunk(chunk: str, model: str, max_tokens: int, temperature: float, debug: bool = False) -> list[str]:
    """
    Constructs the prompt for a transcript chunk, calls the API,
    and returns the topics (split by newline).
    """
    prompt = (
        "Extract up to 3 key topics from the following autogenerated dhamma skype conversation transcript.\n"
        "Format each topic on a new line starting with '1. ..., 2. ..., etc.'.\n"
        "If there are less than 3 topics, don't hallucinate more, just output one or two topics!\n\n"
        "Transcript:\n" + chunk
    )
    if debug:
        print("Prompt for chunk:\n", prompt[:200] + "...")
    response_text = call_api_with_retry(prompt, model, max_tokens, temperature, debug)
    topics = [line.strip() for line in response_text.splitlines() if line.strip()]
    if debug:
        print("Response topics:", topics)
    return topics

def process_transcript(transcript: str, token_threshold: int, model: str, max_tokens: int, temperature: float, debug: bool = False) -> list[str]:
    """
    Processes a single transcript: if it is too long (based on a word-count proxy),
    splits it into chunks and calls the API for each chunk. Then, merges the chunk responses
    (deduplicating topics) to return up to 3 topics.
    """
    token_estimate = len(transcript.split())
    if debug:
        print(f"Transcript estimated token count: {token_estimate}")
    all_topics = []
    if token_estimate > token_threshold:
        words = transcript.split()
        chunks = [" ".join(words[i:i+token_threshold]) for i in range(0, len(words), token_threshold)]
        if debug:
            print(f"Transcript split into {len(chunks)} chunks.")
        for idx, chunk in enumerate(chunks):
            topics = get_topics_for_chunk(chunk, model, max_tokens, temperature, debug)
            all_topics.extend(topics)
    else:
        all_topics = get_topics_for_chunk(transcript, model, max_tokens, temperature, debug)

    # Deduplicate topics while preserving order, and limit to 3 topics.
    seen = set()
    final_topics = []
    for topic in all_topics:
        if topic not in seen:
            seen.add(topic)
            final_topics.append(topic)
        if len(final_topics) >= 3:
            break
    return final_topics

def get_topics_from_transcripts(transcripts: list[str], debug: bool = False) -> list[list[str]]:
    """
    For each transcript, immediately calls the API (or calls for each chunk if needed)
    and returns a list of topic lists.
    """
    model = "gpt-4o-mini-2024-07-18"  # adjust as needed
    max_tokens_output = 250            # expected output tokens per API call
    temperature = 0.3
    TOKEN_THRESHOLD = 10000            # approximate token threshold (using word count as proxy)

    results = []
    for idx, transcript in enumerate(transcripts):
        if debug:
            print(f"\nProcessing transcript {idx} ...")
        topics = process_transcript(transcript, TOKEN_THRESHOLD, model, max_tokens_output, temperature, debug)
        results.append(topics)
    return results

# Example usage:
if __name__ == "__main__":
    transcripts = [
        "This is a short transcript about meditation and mindfulness.",
        "Another transcript that discusses metta practice, anapanasati meditation, and stress reduction techniques.",
        "A very long transcript " + "word " * 12000  # Artificially long transcript to trigger chunking.
    ]
    topics = get_topics_from_transcripts(transcripts, debug=True)
    for i, t in enumerate(topics):
        print(f"Transcript {i} topics:", t)
