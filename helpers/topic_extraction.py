import time
import json
import openai
from openai import OpenAI

client = OpenAI()

def call_api_with_retry(prompt: str, model: str, max_tokens: int, temperature: float, debug: bool = False) -> str:
    """
    Call the ChatCompletion API with retry logic for rate limiting.
    Returns the model's response content as a string.
    """
    retries = 0
    max_retries = 5
    while retries < max_retries:
        try:
            response = client.chat.completions.create(model=model,
            messages=[
                {"role": "system", "content": "You are an AI that extracts key topics from transcript texts."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            temperature=temperature)
            return response.choices[0].message.content.strip()
        except openai.RateLimitError as e:
            sleep_time = 2 ** retries
            if debug:
                print(f"Rate limit hit; retrying in {sleep_time} seconds...")
            time.sleep(sleep_time)
            retries += 1
    raise Exception("API call failed after retries.")

def get_topics_for_chunk(chunk: str, model: str, max_tokens: int, temperature: float, max_number_topics: int, debug: bool = False) -> list[str]:
    """
    Constructs the prompt for a transcript chunk, calls the API,
    and returns the topics (split by newline).
    """

    prompt = (
        "Extract the key topics discussed in the following autogenerated Dhamma Skype conversation transcript.\n"
        f"- Identify up to {max_number_topics} distinct topics, but only include as many as are meaningfully present—do NOT invent additional topics."
        "- Format each topic on a new line as a numbered list (1. ..., 2. ..., etc.).\n"
        "- Be concise and precise, summarizing each topic in a few words or a short phrase.\n"
        "- Accurately interpret foreign words or Dhamma-specific terms based on context.\n"
        "- Avoid repetition — each topic should be unique.\n\n"
        "Transcript:\n" + chunk
    )
    if debug:
        print("Prompt for chunk:\n", prompt[:200] + "...")
    response_text = call_api_with_retry(prompt, model, max_tokens, temperature, debug)
    topics = [line.strip() for line in response_text.splitlines() if line.strip()]
    if debug:
        print("Response topics:", topics)
    return topics

def process_transcript(transcript: str, token_threshold: int, model: str, max_tokens: int, temperature: float, max_number_topics: int, debug: bool = False) -> list[str]:
    """
    Processes a single transcript: if it is too long (based on a word-count proxy),
    splits it into chunks and calls the API for each chunk. Then, merges the chunk responses
    (deduplicating topics) to return up to 3 topics.
    """
    token_estimate = len(transcript.split())
    if debug:
        print(f"Transcript estimated token count: {token_estimate}")
    all_topics = []
    if token_estimate > token_threshold:
        words = transcript.split()
        chunks = [" ".join(words[i:i+token_threshold]) for i in range(0, len(words), token_threshold)]
        # Relative to the chunk length we allocate an amount of topics so that 
        # we end up with max_number_topics when combined
        chunk_sizes = [len(c) for c in chunks]
        num_topics = [max(1, round(max_number_topics*chunk_size/sum(chunk_sizes))) for chunk_size in chunk_sizes]
        if debug:
            print(f"Transcript split into {len(chunks)} chunks.")
        for idx, (chunk, t) in enumerate(zip(chunks, num_topics)):
            topics = get_topics_for_chunk(chunk, model, max_tokens//max_number_topics*t, temperature, t, debug)
            all_topics.extend(topics)
    else:
        all_topics = get_topics_for_chunk(transcript, model, max_tokens, temperature, max_number_topics, debug)

    # Deduplicate topics while preserving order, and limit to 3 topics.
    seen = set()
    final_topics = []
    for topic in all_topics:
        if topic not in seen:
            seen.add(topic)
            final_topics.append(topic)
        if len(final_topics) >= max_number_topics:
            break
    return final_topics

def get_topics_from_transcripts(transcripts: list[str], debug: bool = False) -> list[list[str]]:
    """
    For each transcript, immediately calls the API (or calls for each chunk if needed)
    and returns a list of topic lists.
    """
    model = "gpt-4o-mini-2024-07-18"                           # adjust as needed
    temperature = 0.3
    TOKEN_THRESHOLD = 10000                                    # approximate token threshold (using word count as proxy)
    MAX_NUM_TOPICS_PER_VIDEO = 9
    max_tokens_output = 50*MAX_NUM_TOPICS_PER_VIDEO            # expected output tokens per API call

    results = []
    for idx, transcript in enumerate(transcripts):
        if debug:
            print(f"\nProcessing transcript {idx} ...")
        topics = process_transcript(transcript, TOKEN_THRESHOLD, model, max_tokens_output, temperature, MAX_NUM_TOPICS_PER_VIDEO, debug)
        results.append(topics)
    return results

# Example usage:
if __name__ == "__main__":
    transcripts = [
        "This is a short transcript about meditation and mindfulness.",
        "Another transcript that discusses metta practice, anapanasati meditation, and stress reduction techniques.",
        "A very long transcript " + "word " * 12000  # Artificially long transcript to trigger chunking.
    ]
    topics = get_topics_from_transcripts(transcripts, debug=True)
    for i, t in enumerate(topics):
        print(f"Transcript {i} topics:", t)
